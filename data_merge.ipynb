{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbc598",
   "metadata": {},
   "source": [
    "# Análisis de accidentes con Spark\n",
    "\n",
    "En este laboratorio lo que se busca es poder encotrar información interesante y relevante con respecto a los accidentes que han ocurrido de los años de 2013 a 2023 en Guatemala. Principalmente tenemos 3 tipos de archivos: Fallecidos y Lesionados, Hechos de tránsito y Vehículos involucrados. Estos 3 archivos contienen información con respecto a accidentes automovílisticos. La idea es poder usar spark para poder analizar los eventos y las características relacionadas a este. Lo primero es juntar todos los archivos. Obtuvimos los datos de la página del Instituto Nacional de Estadística (INE): https://www.ine.gob.gt/bases-de-datos/accidentes-de-transito/. Descargando todos los archivos desde 2013 a 2023. El procesamiento se hizo en distintas etapas:\n",
    "\n",
    "1. Descargar todos los datos en un sav y si no esta disponible en ese formato, en excel.\n",
    "2. Juntar todos los dataframes en los 3 tipos de archivos descritos (fallecidos y lesionados, hechos de tránsito y vehículos involucrados)\n",
    "3. Analizar los datos y ver columnas con muchos valores vacíos, columnas repetidas entre otras situaciones que generen data innecesaria o redundante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdfe73",
   "metadata": {},
   "source": [
    "# Merge detodos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb63d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Viendo archivos...\n",
      "\n",
      "hechos_transito:\n",
      "  - hechos_transito2013.sav [SPSS]\n",
      "  - hechos_transito2014.sav [SPSS]\n",
      "  - hechos_transito2015.xlsx [EXCEL]\n",
      "  - hechos_transito2016.sav [SPSS]\n",
      "  - hechos_transito2017.sav [SPSS]\n",
      "  - hechos_transito2018.sav [SPSS]\n",
      "  - hechos_transito2019.sav [SPSS]\n",
      "  - hechos_transito2020.sav [SPSS]\n",
      "  - hechos_transito2021.sav [SPSS]\n",
      "  - hechos_transito2022.sav [SPSS]\n",
      "  - hechos_transito2023.sav [SPSS]\n",
      "\n",
      "vehiculos_involucrados:\n",
      "  - vehiculos_involucrados2013.sav [SPSS]\n",
      "  - vehiculos_involucrados2014.sav [SPSS]\n",
      "  - vehiculos_involucrados2015.xlsx [EXCEL]\n",
      "  - vehiculos_involucrados2016.sav [SPSS]\n",
      "  - vehiculos_involucrados2017.sav [SPSS]\n",
      "  - vehiculos_involucrados2018.sav [SPSS]\n",
      "  - vehiculos_involucrados2019.sav [SPSS]\n",
      "  - vehiculos_involucrados2020.sav [SPSS]\n",
      "  - vehiculos_involucrados2021.sav [SPSS]\n",
      "  - vehiculos_involucrados2022.sav [SPSS]\n",
      "  - vehiculos_involucrados2023.sav [SPSS]\n",
      "\n",
      "fallecidos_lesionados:\n",
      "  - fallecidos_lesionados2013.sav [SPSS]\n",
      "  - fallecidos_lesionados2014.sav [SPSS]\n",
      "  - fallecidos_lesionados2015.xlsx [EXCEL]\n",
      "  - fallecidos_lesionados2016.sav [SPSS]\n",
      "  - fallecidos_lesionados2017.sav [SPSS]\n",
      "  - fallecidos_lesionados2018.sav [SPSS]\n",
      "  - fallecidos_lesionados2019.sav [SPSS]\n",
      "  - fallecidos_lesionados2020.sav [SPSS]\n",
      "  - fallecidos_lesionados2021.sav [SPSS]\n",
      "  - fallecidos_lesionados2022.sav [SPSS]\n",
      "  - fallecidos_lesionados2023.sav [SPSS]\n",
      "\n",
      " Procesando archivos de hechos_transito:\n",
      "  Año 2013: hechos_transito2013.sav\n",
      "   hechos_transito2013.sav: 6324 registros, 20 columnas (SPSS)\n",
      "  Año 2014: hechos_transito2014.sav\n",
      "   hechos_transito2014.sav: 5651 registros, 21 columnas (SPSS)\n",
      "  Año 2015: hechos_transito2015.xlsx\n",
      "   hechos_transito2015.xlsx: 6854 registros, 25 columnas (Excel)\n",
      "  Año 2016: hechos_transito2016.sav\n",
      "   hechos_transito2016.sav: 7964 registros, 18 columnas (SPSS)\n",
      "  Año 2017: hechos_transito2017.sav\n",
      "   hechos_transito2017.sav: 5879 registros, 17 columnas (SPSS)\n",
      "  Año 2018: hechos_transito2018.sav\n",
      "   hechos_transito2018.sav: 6395 registros, 17 columnas (SPSS)\n",
      "  Año 2019: hechos_transito2019.sav\n",
      "   hechos_transito2019.sav: 7047 registros, 17 columnas (SPSS)\n",
      "  Año 2020: hechos_transito2020.sav\n",
      "   hechos_transito2020.sav: 6350 registros, 17 columnas (SPSS)\n",
      "  Año 2021: hechos_transito2021.sav\n",
      "   hechos_transito2021.sav: 8153 registros, 18 columnas (SPSS)\n",
      "  Año 2022: hechos_transito2022.sav\n",
      "   hechos_transito2022.sav: 7924 registros, 17 columnas (SPSS)\n",
      "  Año 2023: hechos_transito2023.sav\n",
      "   hechos_transito2023.sav: 8218 registros, 17 columnas (SPSS)\n",
      "\n",
      " hechos_transito - Resultado:\n",
      "   - Total registros: 76759\n",
      "   - Total columnas: 49\n",
      "   - Años incluidos: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "\n",
      " Procesando archivos de vehiculos_involucrados:\n",
      "  Año 2013: vehiculos_involucrados2013.sav\n",
      "   vehiculos_involucrados2013.sav: 6323 registros, 20 columnas (SPSS)\n",
      "  Año 2014: vehiculos_involucrados2014.sav\n",
      "   vehiculos_involucrados2014.sav: 7904 registros, 21 columnas (SPSS)\n",
      "  Año 2015: vehiculos_involucrados2015.xlsx\n",
      "   vehiculos_involucrados2015.xlsx: 9823 registros, 24 columnas (Excel)\n",
      "  Año 2016: vehiculos_involucrados2016.sav\n",
      "   vehiculos_involucrados2016.sav: 11618 registros, 25 columnas (SPSS)\n",
      "  Año 2017: vehiculos_involucrados2017.sav\n",
      "   vehiculos_involucrados2017.sav: 8644 registros, 24 columnas (SPSS)\n",
      "  Año 2018: vehiculos_involucrados2018.sav\n",
      "   vehiculos_involucrados2018.sav: 9514 registros, 24 columnas (SPSS)\n",
      "  Año 2019: vehiculos_involucrados2019.sav\n",
      "   vehiculos_involucrados2019.sav: 10827 registros, 24 columnas (SPSS)\n",
      "  Año 2020: vehiculos_involucrados2020.sav\n",
      "   vehiculos_involucrados2020.sav: 10103 registros, 24 columnas (SPSS)\n",
      "  Año 2021: vehiculos_involucrados2021.sav\n",
      "   vehiculos_involucrados2021.sav: 12796 registros, 25 columnas (SPSS)\n",
      "  Año 2022: vehiculos_involucrados2022.sav\n",
      "   vehiculos_involucrados2022.sav: 12239 registros, 24 columnas (SPSS)\n",
      "  Año 2023: vehiculos_involucrados2023.sav\n",
      "   vehiculos_involucrados2023.sav: 12197 registros, 24 columnas (SPSS)\n",
      "\n",
      " vehiculos_involucrados - Resultado:\n",
      "   - Total registros: 111988\n",
      "   - Total columnas: 49\n",
      "   - Años incluidos: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "\n",
      " Procesando archivos de fallecidos_lesionados:\n",
      "  Año 2013: fallecidos_lesionados2013.sav\n",
      "   fallecidos_lesionados2013.sav: 9060 registros, 22 columnas (SPSS)\n",
      "  Año 2014: fallecidos_lesionados2014.sav\n",
      "   fallecidos_lesionados2014.sav: 8990 registros, 23 columnas (SPSS)\n",
      "  Año 2015: fallecidos_lesionados2015.xlsx\n",
      "   fallecidos_lesionados2015.xlsx: 10397 registros, 26 columnas (Excel)\n",
      "  Año 2016: fallecidos_lesionados2016.sav\n",
      "   fallecidos_lesionados2016.sav: 11668 registros, 27 columnas (SPSS)\n",
      "  Año 2017: fallecidos_lesionados2017.sav\n",
      "   fallecidos_lesionados2017.sav: 8625 registros, 25 columnas (SPSS)\n",
      "  Año 2018: fallecidos_lesionados2018.sav\n",
      "   fallecidos_lesionados2018.sav: 9407 registros, 25 columnas (SPSS)\n",
      "  Año 2019: fallecidos_lesionados2019.sav\n",
      "   fallecidos_lesionados2019.sav: 10664 registros, 25 columnas (SPSS)\n",
      "  Año 2020: fallecidos_lesionados2020.sav\n",
      "   fallecidos_lesionados2020.sav: 8142 registros, 25 columnas (SPSS)\n",
      "  Año 2021: fallecidos_lesionados2021.sav\n",
      "   fallecidos_lesionados2021.sav: 10544 registros, 26 columnas (SPSS)\n",
      "  Año 2022: fallecidos_lesionados2022.sav\n",
      "   fallecidos_lesionados2022.sav: 10722 registros, 25 columnas (SPSS)\n",
      "  Año 2023: fallecidos_lesionados2023.sav\n",
      "   fallecidos_lesionados2023.sav: 11198 registros, 25 columnas (SPSS)\n",
      "\n",
      " fallecidos_lesionados - Resultado:\n",
      "   - Total registros: 109417\n",
      "   - Total columnas: 48\n",
      "   - Años incluidos: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "\n",
      " Guardando datasets unidos...\n",
      " hechos_transito guardado en: datasets_unidos\\hechos_transito_completo.csv\n",
      " vehiculos_involucrados guardado en: datasets_unidos\\vehiculos_involucrados_completo.csv\n",
      " fallecidos_lesionados guardado en: datasets_unidos\\fallecidos_lesionados_completo.csv\n",
      "\n",
      " PROCESO COMPLETADO\n",
      "========================================\n",
      "hechos_transito: 76,759 registros, 49 columnas\n",
      "     Años: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "vehiculos_involucrados: 111,988 registros, 49 columnas\n",
      "     Años: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "fallecidos_lesionados: 109,417 registros, 48 columnas\n",
      "     Años: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def detectar_formato_archivos(directorio):\n",
    "    \"\"\"Detecta los formatos de archivo disponibles por tipo\"\"\"\n",
    "    tipos = {\n",
    "        'hechos_transito': [],\n",
    "        'vehiculos_involucrados': [],\n",
    "        'fallecidos_lesionados': []\n",
    "    }\n",
    "    \n",
    "    for archivo in Path(directorio).iterdir():\n",
    "        # Manejar extensiones dobles como .sav.xlsx\n",
    "        if archivo.suffix in ['.sav', '.xlsx'] or '.sav.xlsx' in archivo.name:\n",
    "            nombre = archivo.stem.lower()\n",
    "            # Remover .sav del nombre si existe extensión doble\n",
    "            nombre = nombre.replace('.sav', '')\n",
    "            \n",
    "            if 'hechos_transito' in nombre:\n",
    "                tipos['hechos_transito'].append(archivo)\n",
    "            elif 'vehiculos_involucrados' in nombre:\n",
    "                tipos['vehiculos_involucrados'].append(archivo)\n",
    "            elif 'fallecidos_lesionados' in nombre:\n",
    "                tipos['fallecidos_lesionados'].append(archivo)\n",
    "    \n",
    "    return tipos\n",
    "\n",
    "def determinar_formato_archivo(archivo):\n",
    "    \"\"\"Determina el formato real del archivo\"\"\"\n",
    "    nombre = archivo.name.lower()\n",
    "    \n",
    "    if nombre.endswith('.sav'):\n",
    "        return 'spss'\n",
    "    elif nombre.endswith('.xlsx') or nombre.endswith('.sav.xlsx'):\n",
    "        return 'excel'\n",
    "    else:\n",
    "        return 'desconocido'\n",
    "\n",
    "def leer_archivo(archivo):\n",
    "    \"\"\"Lee un archivo según su formato (SPSS o Excel)\"\"\"\n",
    "    try:\n",
    "        formato = determinar_formato_archivo(archivo)\n",
    "        \n",
    "        if formato == 'spss':\n",
    "            df, meta = pyreadstat.read_sav(archivo)\n",
    "            print(f\"   {archivo.name}: {len(df)} registros, {len(df.columns)} columnas (SPSS)\")\n",
    "            return df\n",
    "        elif formato == 'excel':\n",
    "            df = pd.read_excel(archivo)\n",
    "            print(f\"   {archivo.name}: {len(df)} registros, {len(df.columns)} columnas (Excel)\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"   Formato no soportado: {archivo.name}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Error leyendo {archivo.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extraer_anio(nombre_archivo):\n",
    "    \"\"\"Extrae el año del nombre del archivo\"\"\"\n",
    "    # Primero limpiar el nombre de extensiones dobles\n",
    "    nombre_limpio = nombre_archivo.stem.replace('.sav', '').replace('.xlsx', '')\n",
    "    match = re.search(r'(\\d{4})', nombre_limpio)\n",
    "    return match.group(1) if match else \"Desconocido\"\n",
    "\n",
    "def unir_archivos_por_tipo(lista_archivos, tipo):\n",
    "    \"\"\"Une archivos del mismo tipo manejando discrepancias de columnas\"\"\"\n",
    "    print(f\"\\n Procesando archivos de {tipo}:\")\n",
    "    \n",
    "    dataframes = []\n",
    "    problemas = []\n",
    "    \n",
    "    for archivo in sorted(lista_archivos):\n",
    "        año = extraer_anio(archivo)\n",
    "        print(f\"  Año {año}: {archivo.name}\")\n",
    "        \n",
    "        df = leer_archivo(archivo)\n",
    "        if df is not None:\n",
    "            # Agregar metadatos\n",
    "            df['año'] = año\n",
    "            df['archivo_origen'] = archivo.name\n",
    "            df['tipo_dataset'] = tipo\n",
    "            \n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            problemas.append(archivo.name)\n",
    "    \n",
    "    if dataframes:\n",
    "        # Unir todos los DataFrames manejando columnas diferentes\n",
    "        df_final = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        print(f\"\\n {tipo} - Resultado:\")\n",
    "        print(f\"   - Total registros: {len(df_final)}\")\n",
    "        print(f\"   - Total columnas: {len(df_final.columns)}\")\n",
    "        print(f\"   - Años incluidos: {sorted(df_final['año'].unique())}\")\n",
    "        \n",
    "        return df_final\n",
    "    else:\n",
    "        print(f\" No se pudo procesar ningún archivo de {tipo}\")\n",
    "        return None\n",
    "\n",
    "def analizar_estructura_columnas(dfs_unidos):\n",
    "    \"\"\"Analiza las columnas comunes y diferentes entre datasets\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" ANÁLISIS DE ESTRUCTURA DE COLUMNAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for tipo, df in dfs_unidos.items():\n",
    "        if df is not None:\n",
    "            print(f\"\\n{tipo.upper()}:\")\n",
    "            print(f\"  Columnas ({len(df.columns)}): {list(df.columns)}\")\n",
    "            \n",
    "            # Verificar valores nulos por columna\n",
    "            print(f\"  Valores nulos por columna:\")\n",
    "            for col in df.columns:\n",
    "                nulos = df[col].isna().sum()\n",
    "                if nulos > 0:\n",
    "                    print(f\"    - {col}: {nulos} nulos ({nulos/len(df)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Configuración\n",
    "directorio_datos = \"./data\"\n",
    "directorio_salida = \"./datasets_unidos\"\n",
    "\n",
    "print(\" Viendo archivos...\")\n",
    "tipos_archivos = detectar_formato_archivos(directorio_datos)\n",
    "\n",
    "# Mostrar archivos encontrados\n",
    "for tipo, archivos in tipos_archivos.items():\n",
    "    print(f\"\\n{tipo}:\")\n",
    "    for archivo in sorted(archivos):\n",
    "        formato = determinar_formato_archivo(archivo)\n",
    "        print(f\"  - {archivo.name} [{formato.upper()}]\")\n",
    "\n",
    "# Unir archivos por tipo\n",
    "dfs_unidos = {}\n",
    "for tipo, archivos in tipos_archivos.items():\n",
    "    if archivos:\n",
    "        dfs_unidos[tipo] = unir_archivos_por_tipo(archivos, tipo)\n",
    "    else:\n",
    "        print(f\"\\n  No se encontraron archivos para {tipo}\")\n",
    "        dfs_unidos[tipo] = None\n",
    "\n",
    "# Guardar resultados\n",
    "print(f\"\\n Guardando datasets unidos...\")\n",
    "Path(directorio_salida).mkdir(exist_ok=True)\n",
    "\n",
    "for tipo, df in dfs_unidos.items():\n",
    "    if df is not None:\n",
    "        archivo_salida = Path(directorio_salida) / f\"{tipo}_completo.csv\"\n",
    "        df.to_csv(archivo_salida, index=False, encoding='utf-8-sig')\n",
    "        print(f\" {tipo} guardado en: {archivo_salida}\")\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n PROCESO COMPLETADO\")\n",
    "print(\"=\"*40)\n",
    "for tipo, df in dfs_unidos.items():\n",
    "    if df is not None:\n",
    "        print(f\"{tipo}: {len(df):,} registros, {len(df.columns)} columnas\")\n",
    "        print(f\"     Años: {sorted(df['año'].unique())}\")\n",
    "    else:\n",
    "        print(f\"{tipo}: No procesado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf3505",
   "metadata": {},
   "source": [
    "se puede ver que se han logrado juntar la información de todos los años, incluso teniendo archivos de distinto tipo. Ahora es encesario evaluar las columnas y ver cuales podrían no ser necesaríass o incluso problemáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e680a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ANÁLISIS DE ESTRUCTURA DE COLUMNAS\n",
      "============================================================\n",
      "\n",
      "HECHOS_TRANSITO:\n",
      "  Columnas (49): ['num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu', 'g_hora', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'g_edad_2', 'mayor_menor', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'marca_veh', 'estado_pil', 'año', 'archivo_origen', 'tipo_dataset', 'num_correlativo', 'corre_base', 'día_ocu', 'día_sem_ocu', 'área_geo_ocu', 'sexo_con', 'edad_con', 'g_edad', 'estado_con', 'tipo_eve', 'núm_corre', 'año_ocu', 'g_hora_5', 'sexo_per', 'edad_per', 'g_edad_80ymás', 'g_edad_60ymás', 'edad_quinquenales', 'g_modelo_veh', 'Núm_corre', 'Año_ocu', 'Día_ocu', 'Hora_ocu', 'Mes_ocu', 'zona_ciudad', 'num_corre']\n",
      "  Valores nulos por columna:\n",
      "    - num_hecho: 70435 nulos (91.8%)\n",
      "    - dia_ocu: 70435 nulos (91.8%)\n",
      "    - mes_ocu: 6350 nulos (8.3%)\n",
      "    - dia_sem_ocu: 70435 nulos (91.8%)\n",
      "    - hora_ocu: 6350 nulos (8.3%)\n",
      "    - areag_ocu: 70435 nulos (91.8%)\n",
      "    - sexo_pil: 70435 nulos (91.8%)\n",
      "    - edad_pil: 70435 nulos (91.8%)\n",
      "    - g_edad_2: 70435 nulos (91.8%)\n",
      "    - mayor_menor: 57930 nulos (75.5%)\n",
      "    - causa_acc: 70435 nulos (91.8%)\n",
      "    - estado_pil: 70435 nulos (91.8%)\n",
      "    - num_correlativo: 71108 nulos (92.6%)\n",
      "    - corre_base: 71108 nulos (92.6%)\n",
      "    - día_ocu: 12674 nulos (16.5%)\n",
      "    - día_sem_ocu: 6324 nulos (8.2%)\n",
      "    - área_geo_ocu: 56290 nulos (73.3%)\n",
      "    - sexo_con: 71108 nulos (92.6%)\n",
      "    - edad_con: 71108 nulos (92.6%)\n",
      "    - g_edad: 71108 nulos (92.6%)\n",
      "    - estado_con: 64254 nulos (83.7%)\n",
      "    - tipo_eve: 6324 nulos (8.2%)\n",
      "    - núm_corre: 26249 nulos (34.2%)\n",
      "    - año_ocu: 18325 nulos (23.9%)\n",
      "    - g_hora_5: 11975 nulos (15.6%)\n",
      "    - sexo_per: 69905 nulos (91.1%)\n",
      "    - edad_per: 69905 nulos (91.1%)\n",
      "    - g_edad_80ymás: 69905 nulos (91.1%)\n",
      "    - g_edad_60ymás: 69905 nulos (91.1%)\n",
      "    - edad_quinquenales: 69905 nulos (91.1%)\n",
      "    - g_modelo_veh: 11975 nulos (15.6%)\n",
      "    - Núm_corre: 70409 nulos (91.7%)\n",
      "    - Año_ocu: 70409 nulos (91.7%)\n",
      "    - Día_ocu: 70409 nulos (91.7%)\n",
      "    - Hora_ocu: 70409 nulos (91.7%)\n",
      "    - Mes_ocu: 70409 nulos (91.7%)\n",
      "    - zona_ciudad: 68606 nulos (89.4%)\n",
      "    - num_corre: 68835 nulos (89.7%)\n",
      "\n",
      "VEHICULOS_INVOLUCRADOS:\n",
      "  Columnas (49): ['num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'mayor_menor', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'marca_veh', 'g_edad', 'estado_pil', 'grupo_mode_veh', 'año', 'archivo_origen', 'tipo_dataset', 'num_correlativo', 'corre_base', 'día_ocu', 'día_sem_ocu', 'g_hora', 'área_geo_ocu', 'sexo_con', 'edad_con', 'estado_con', 'tipo_eve', 'núm_corre', 'g_hora_5', 'sexo_per', 'edad_per', 'g_edad_80ymás', 'g_edad_60ymás', 'edad_quinquenales', 'g_modelo_veh', 'num_corre', 'año_ocu', 'Núm_corre', 'Año_ocu', 'Día_ocu', 'Hora_ocu', 'Mes_ocu', 'zona_ciudad']\n",
      "  Valores nulos por columna:\n",
      "    - num_hecho: 105665 nulos (94.4%)\n",
      "    - dia_ocu: 105665 nulos (94.4%)\n",
      "    - mes_ocu: 10103 nulos (9.0%)\n",
      "    - dia_sem_ocu: 105665 nulos (94.4%)\n",
      "    - hora_ocu: 10103 nulos (9.0%)\n",
      "    - areag_ocu: 105665 nulos (94.4%)\n",
      "    - sexo_pil: 105665 nulos (94.4%)\n",
      "    - edad_pil: 105665 nulos (94.4%)\n",
      "    - causa_acc: 105665 nulos (94.4%)\n",
      "    - g_edad: 97761 nulos (87.3%)\n",
      "    - estado_pil: 105665 nulos (94.4%)\n",
      "    - grupo_mode_veh: 105665 nulos (94.4%)\n",
      "    - num_correlativo: 104084 nulos (92.9%)\n",
      "    - corre_base: 104084 nulos (92.9%)\n",
      "    - día_ocu: 16426 nulos (14.7%)\n",
      "    - día_sem_ocu: 6323 nulos (5.6%)\n",
      "    - g_hora: 6323 nulos (5.6%)\n",
      "    - área_geo_ocu: 82643 nulos (73.8%)\n",
      "    - sexo_con: 104084 nulos (92.9%)\n",
      "    - edad_con: 104084 nulos (92.9%)\n",
      "    - estado_con: 6323 nulos (5.6%)\n",
      "    - tipo_eve: 6323 nulos (5.6%)\n",
      "    - núm_corre: 35948 nulos (32.1%)\n",
      "    - g_hora_5: 14227 nulos (12.7%)\n",
      "    - sexo_per: 14227 nulos (12.7%)\n",
      "    - edad_per: 14227 nulos (12.7%)\n",
      "    - g_edad_80ymás: 14227 nulos (12.7%)\n",
      "    - g_edad_60ymás: 14227 nulos (12.7%)\n",
      "    - edad_quinquenales: 14227 nulos (12.7%)\n",
      "    - g_modelo_veh: 14227 nulos (12.7%)\n",
      "    - num_corre: 100370 nulos (89.6%)\n",
      "    - año_ocu: 34153 nulos (30.5%)\n",
      "    - Núm_corre: 101885 nulos (91.0%)\n",
      "    - Año_ocu: 101885 nulos (91.0%)\n",
      "    - Día_ocu: 101885 nulos (91.0%)\n",
      "    - Hora_ocu: 101885 nulos (91.0%)\n",
      "    - Mes_ocu: 101885 nulos (91.0%)\n",
      "    - zona_ciudad: 99192 nulos (88.6%)\n",
      "\n",
      "FALLECIDOS_LESIONADOS:\n",
      "  Columnas (48): ['num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu', 'g_hora', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'g_edad_2', 'mayor_menor', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'marca_veh', 'g_edad', 'Fallecidos_Lesionados', 'Otro_g_edad_fall_les', 'año', 'archivo_origen', 'tipo_dataset', 'num_correlativo', 'num_corre', 'corre_base', 'día_ocu', 'día_sem_ocu', 'área_geo_ocu', 'sexo_víc', 'edad_víc', 'fall_les', 'tipo_eve', 'edad_quinquenales', 'núm_corre', 'año_ocu', 'g_hora_5', 'sexo_per', 'edad_per', 'g_edad_80ymás', 'g_edad_60ymás', 'int_o_noint', 'g_modelo_veh', 'filter_$', 'Núm_corre', 'zona_ciudad']\n",
      "  Valores nulos por columna:\n",
      "    - num_hecho: 100357 nulos (91.7%)\n",
      "    - dia_ocu: 100357 nulos (91.7%)\n",
      "    - dia_sem_ocu: 100357 nulos (91.7%)\n",
      "    - areag_ocu: 100357 nulos (91.7%)\n",
      "    - sexo_pil: 100357 nulos (91.7%)\n",
      "    - edad_pil: 100357 nulos (91.7%)\n",
      "    - g_edad_2: 100357 nulos (91.7%)\n",
      "    - causa_acc: 100357 nulos (91.7%)\n",
      "    - g_edad: 91367 nulos (83.5%)\n",
      "    - Fallecidos_Lesionados: 100357 nulos (91.7%)\n",
      "    - Otro_g_edad_fall_les: 100357 nulos (91.7%)\n",
      "    - num_correlativo: 100427 nulos (91.8%)\n",
      "    - num_corre: 89805 nulos (82.1%)\n",
      "    - corre_base: 100427 nulos (91.8%)\n",
      "    - día_ocu: 9060 nulos (8.3%)\n",
      "    - día_sem_ocu: 9060 nulos (8.3%)\n",
      "    - área_geo_ocu: 78362 nulos (71.6%)\n",
      "    - sexo_víc: 100427 nulos (91.8%)\n",
      "    - edad_víc: 100427 nulos (91.8%)\n",
      "    - fall_les: 9060 nulos (8.3%)\n",
      "    - tipo_eve: 9060 nulos (8.3%)\n",
      "    - edad_quinquenales: 9060 nulos (8.3%)\n",
      "    - núm_corre: 36914 nulos (33.7%)\n",
      "    - año_ocu: 18050 nulos (16.5%)\n",
      "    - g_hora_5: 18050 nulos (16.5%)\n",
      "    - sexo_per: 18050 nulos (16.5%)\n",
      "    - edad_per: 18050 nulos (16.5%)\n",
      "    - g_edad_80ymás: 18050 nulos (16.5%)\n",
      "    - g_edad_60ymás: 18050 nulos (16.5%)\n",
      "    - int_o_noint: 18050 nulos (16.5%)\n",
      "    - g_modelo_veh: 18050 nulos (16.5%)\n",
      "    - filter_$: 97749 nulos (89.3%)\n",
      "    - Núm_corre: 101275 nulos (92.6%)\n",
      "    - zona_ciudad: 98873 nulos (90.4%)\n"
     ]
    }
   ],
   "source": [
    "analizar_estructura_columnas(dfs_unidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c808e0",
   "metadata": {},
   "source": [
    "Viendo estos resultados es claro que hay algunos problemas, tenemos más de 40 columnnas en los 3 archivos y varias de esas columnas tienen valores nulos. Es más hay algunas columnas que son literalmente lo mismo, pero debido a ortografía (espacios, tildes, diferencias en el nombre) hay columnas que se marcan como diferente. Sabiendo esto es necesario limitar la selección de datos y jutnar la información redundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d60a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets unidos...\n",
      "Datasets cargados:\n",
      "   - Hechos: 76,759 registros, 49 columnas\n",
      "   - Vehiculos: 111,988 registros, 49 columnas\n",
      "   - Fallecidos: 109,417 registros, 48 columnas\n",
      "\n",
      "Estandarizando nombres de columnas...\n",
      "   Nombres de columnas estandarizados\n",
      "\n",
      "Eliminando columnas duplicadas...\n",
      "   Eliminando columnas duplicadas...\n",
      "     Encontradas 8 columnas duplicadas: ['dia_ocu', 'dia_sem_ocu', 'num_corre', 'ano_ocu', 'dia_ocu', 'hora_ocu', 'mes_ocu', 'num_corre']\n",
      "     Columnas duplicadas eliminadas\n",
      "   Eliminando columnas duplicadas...\n",
      "     Encontradas 8 columnas duplicadas: ['dia_ocu', 'dia_sem_ocu', 'num_corre', 'num_corre', 'ano_ocu', 'dia_ocu', 'hora_ocu', 'mes_ocu']\n",
      "     Columnas duplicadas eliminadas\n",
      "   Eliminando columnas duplicadas...\n",
      "     Encontradas 4 columnas duplicadas: ['dia_ocu', 'dia_sem_ocu', 'num_corre', 'num_corre']\n",
      "     Columnas duplicadas eliminadas\n",
      "\n",
      "Combinando columnas similares...\n",
      "   Combinando columnas similares...\n",
      "     Combinando ['num_hecho', 'num_correlativo', 'num_corre'] en num_hecho\n",
      "     Combinando ['areag_ocu', 'area_geo_ocu'] en areag_ocu\n",
      "     Combinando ['zona_ocu', 'zona_ciudad'] en zona_ocu\n",
      "     Combinando ['sexo_pil', 'sexo_con', 'sexo_per'] en sexo_pil\n",
      "     Combinando ['edad_pil', 'edad_con', 'edad_per'] en edad_pil\n",
      "     Combinando ['estado_pil', 'estado_con'] en estado_pil\n",
      "   Combinando columnas similares...\n",
      "     Combinando ['num_hecho', 'num_correlativo', 'num_corre'] en num_hecho\n",
      "     Combinando ['areag_ocu', 'area_geo_ocu'] en areag_ocu\n",
      "     Combinando ['zona_ocu', 'zona_ciudad'] en zona_ocu\n",
      "     Combinando ['sexo_pil', 'sexo_con', 'sexo_per'] en sexo_pil\n",
      "     Combinando ['edad_pil', 'edad_con', 'edad_per'] en edad_pil\n",
      "     Combinando ['estado_pil', 'estado_con'] en estado_pil\n",
      "   Combinando columnas similares...\n",
      "     Combinando ['num_hecho', 'num_correlativo', 'num_corre'] en num_hecho\n",
      "     Combinando ['areag_ocu', 'area_geo_ocu'] en areag_ocu\n",
      "     Combinando ['zona_ocu', 'zona_ciudad'] en zona_ocu\n",
      "     Combinando ['sexo_pil', 'sexo_per'] en sexo_pil\n",
      "     Combinando ['edad_pil', 'edad_per'] en edad_pil\n",
      "     Combinando ['fallecidos_lesionados', 'fall_les'] en fallecidos_lesionados\n",
      "\n",
      "Limpiando y seleccionando columnas relevantes...\n",
      "Limpiando dataset: HECHOS_TRANSITO\n",
      "   - Columna 'año' extraída de 'archivo_origen'\n",
      "   - Columnas originales: 32\n",
      "   - Columnas finales: 18\n",
      "   - Registros originales: 76759\n",
      "   - Registros despues de eliminar duplicados: 74749\n",
      "   - Columnas seleccionadas: ['num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'archivo_origen', 'tipo_dataset', 'año']\n",
      "Limpiando dataset: VEHICULOS_INVOLUCRADOS\n",
      "   - Columna 'año' extraída de 'archivo_origen'\n",
      "   - Columnas originales: 32\n",
      "   - Columnas finales: 19\n",
      "   - Registros originales: 111988\n",
      "   - Registros despues de eliminar duplicados: 111360\n",
      "   - Columnas seleccionadas: ['num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'marca_veh', 'estado_pil', 'archivo_origen', 'tipo_dataset', 'año']\n",
      "Limpiando dataset: FALLECIDOS_LESIONADOS\n",
      "   - Columna 'año' extraída de 'archivo_origen'\n",
      "   - Columnas originales: 37\n",
      "   - Columnas finales: 17\n",
      "   - Registros originales: 109417\n",
      "   - Registros despues de eliminar duplicados: 106879\n",
      "   - Columnas seleccionadas: ['num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'causa_acc', 'fallecidos_lesionados', 'archivo_origen', 'tipo_dataset', 'tipo_eve', 'año']\n",
      "\n",
      "============================================================\n",
      "ANALISIS DE CALIDAD POST-LIMPIEZA\n",
      "============================================================\n",
      "CALIDAD DE DATOS - HECHOS_TRANSITO:\n",
      "   - Total registros: 74,749\n",
      "   - Total columnas: 18\n",
      "   - Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   - Valores nulos por columna:\n",
      "     - num_hecho: 12,264 nulos (16.4%)\n",
      "     - dia_ocu: 68,425 nulos (91.5%)\n",
      "     - mes_ocu: 4,421 nulos (5.9%)\n",
      "     - dia_sem_ocu: 68,425 nulos (91.5%)\n",
      "     - hora_ocu: 4,421 nulos (5.9%)\n",
      "     - areag_ocu: 47,956 nulos (64.2%)\n",
      "     - sexo_pil: 55,920 nulos (74.8%)\n",
      "     - edad_pil: 55,920 nulos (74.8%)\n",
      "     - causa_acc: 68,425 nulos (91.5%)\n",
      "CALIDAD DE DATOS - VEHICULOS_INVOLUCRADOS:\n",
      "   - Total registros: 111,360\n",
      "   - Total columnas: 19\n",
      "   - Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   - Valores nulos por columna:\n",
      "     - num_hecho: 21,093 nulos (18.9%)\n",
      "     - dia_ocu: 105,037 nulos (94.3%)\n",
      "     - mes_ocu: 9,536 nulos (8.6%)\n",
      "     - hora_ocu: 9,536 nulos (8.6%)\n",
      "     - areag_ocu: 75,753 nulos (68.0%)\n",
      "     - causa_acc: 105,037 nulos (94.3%)\n",
      "CALIDAD DE DATOS - FALLECIDOS_LESIONADOS:\n",
      "   - Total registros: 106,879\n",
      "   - Total columnas: 17\n",
      "   - Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   - Valores nulos por columna:\n",
      "     - num_hecho: 78,107 nulos (73.1%)\n",
      "     - dia_ocu: 97,819 nulos (91.5%)\n",
      "     - areag_ocu: 67,508 nulos (63.2%)\n",
      "     - sexo_pil: 8,990 nulos (8.4%)\n",
      "     - edad_pil: 8,990 nulos (8.4%)\n",
      "     - causa_acc: 97,819 nulos (91.5%)\n",
      "     - tipo_eve: 9,060 nulos (8.5%)\n",
      "\n",
      "Guardando datasets limpios...\n",
      "Datasets limpios guardados en: ./datasets_limpios\n",
      "\n",
      "ESTRUCTURA FINAL:\n",
      "   Hechos transito: ['num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'archivo_origen', 'tipo_dataset', 'año']\n",
      "   Vehiculos: ['num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc', 'marca_veh', 'estado_pil', 'archivo_origen', 'tipo_dataset', 'año']\n",
      "   Fallecidos/Lesionados: ['num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu', 'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', 'sexo_pil', 'edad_pil', 'tipo_veh', 'causa_acc', 'fallecidos_lesionados', 'archivo_origen', 'tipo_dataset', 'tipo_eve', 'año']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def estandarizar_nombres_columnas(df):\n",
    "    \"\"\"Estandariza nombres de columnas: minusculas, sin tildes, sin espacios\"\"\"\n",
    "    def limpiar_nombre(col):\n",
    "        # Convertir a string y minusculas\n",
    "        col = str(col).lower()\n",
    "        # Remover tildes\n",
    "        col = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('ASCII')\n",
    "        # Reemplazar espacios y caracteres especiales\n",
    "        col = col.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
    "        return col\n",
    "    \n",
    "    new_columns = [limpiar_nombre(col) for col in df.columns]\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "def eliminar_columnas_duplicadas(df):\n",
    "    \"\"\"Elimina columnas duplicadas por nombre\"\"\"\n",
    "    print(\"   Eliminando columnas duplicadas...\")\n",
    "    \n",
    "    # Identificar columnas duplicadas\n",
    "    columnas_duplicadas = df.columns[df.columns.duplicated()].tolist()\n",
    "    \n",
    "    if columnas_duplicadas:\n",
    "        print(f\"     Encontradas {len(columnas_duplicadas)} columnas duplicadas: {columnas_duplicadas}\")\n",
    "        # Mantener solo la primera ocurrencia de cada columna\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        print(f\"     Columnas duplicadas eliminadas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extraer_ano_archivo(archivo_str):\n",
    "    \"\"\"Extrae el año del string del archivo\"\"\"\n",
    "    # Buscar un patrón de 4 dígitos (año)\n",
    "    match = re.search(r'(\\d{4})', archivo_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def combinar_columnas_similares(df):\n",
    "    \"\"\"\n",
    "    Combina columnas similares para reducir valores nulos.\n",
    "    Asume que los nombres ya están estandarizados.\n",
    "    \"\"\"\n",
    "    print(\"   Combinando columnas similares...\")\n",
    "    \n",
    "    # Mapeo de columnas a consolidar. La clave es el nombre de la columna objetivo.\n",
    "    mapeo = {\n",
    "        'num_hecho': ['num_hecho', 'num_correlativo', 'num_corre'],\n",
    "        'dia_ocu': ['dia_ocu'],\n",
    "        'mes_ocu': ['mes_ocu'],\n",
    "        'hora_ocu': ['hora_ocu'],\n",
    "        'dia_sem_ocu': ['dia_sem_ocu'],\n",
    "        'depto_ocu': ['depto_ocu'],\n",
    "        'mupio_ocu': ['mupio_ocu'],\n",
    "        'areag_ocu': ['areag_ocu', 'area_geo_ocu'],\n",
    "        'zona_ocu': ['zona_ocu', 'zona_ciudad'],\n",
    "        'sexo_pil': ['sexo_pil', 'sexo_con', 'sexo_per'],\n",
    "        'edad_pil': ['edad_pil', 'edad_con', 'edad_per'],\n",
    "        'tipo_veh': ['tipo_veh'],\n",
    "        'color_veh': ['color_veh'],\n",
    "        'modelo_veh': ['modelo_veh'],\n",
    "        'causa_acc': ['causa_acc'],\n",
    "        'marca_veh': ['marca_veh'],\n",
    "        'estado_pil': ['estado_pil', 'estado_con'],\n",
    "        'fallecidos_lesionados': ['fallecidos_lesionados', 'fall_les'],\n",
    "        'tipo_eve': ['tipo_eve']\n",
    "    }\n",
    "    \n",
    "    for col_principal, columnas in mapeo.items():\n",
    "        # Filtrar las columnas que existen en el DataFrame\n",
    "        columnas_existentes = [col for col in columnas if col in df.columns]\n",
    "        if len(columnas_existentes) > 1:\n",
    "            print(f\"     Combinando {columnas_existentes} en {col_principal}\")\n",
    "            # Si la columna principal no está en el DataFrame, usamos la primera de las existentes\n",
    "            if col_principal not in df.columns:\n",
    "                col_principal = columnas_existentes[0]\n",
    "                # Si la columna principal no es la primera, la renombramos\n",
    "                if col_principal != columnas_existentes[0]:\n",
    "                    df = df.rename(columns={col_principal: columnas_existentes[0]})\n",
    "                    col_principal = columnas_existentes[0]\n",
    "            # Para cada columna similar (excluyendo la principal)\n",
    "            for col in columnas_existentes:\n",
    "                if col != col_principal:\n",
    "                    # Combinar: donde la principal tiene nulos y la similar tiene valor, copiamos el valor.\n",
    "                    mask = df[col_principal].isna() & df[col].notna()\n",
    "                    df.loc[mask, col_principal] = df.loc[mask, col]\n",
    "                    # Eliminamos la columna similar\n",
    "                    df = df.drop(columns=[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpiar_y_seleccionar_columnas(df, tipo_dataset):\n",
    "    \"\"\"Funcion generica para limpiar y seleccionar columnas relevantes\"\"\"\n",
    "    print(f\"Limpiando dataset: {tipo_dataset.upper()}\")\n",
    "    \n",
    "    # Definir columnas relevantes segun el tipo de dataset\n",
    "    if tipo_dataset == 'hechos_transito':\n",
    "        columnas_relevantes = [\n",
    "            'num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu',\n",
    "            'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', \n",
    "            'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc',\n",
    "            'sexo_pil', 'edad_pil'\n",
    "        ]\n",
    "    elif tipo_dataset == 'vehiculos_involucrados':\n",
    "        columnas_relevantes = [\n",
    "            'num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu',\n",
    "            'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu',\n",
    "            'tipo_veh', 'color_veh', 'modelo_veh', 'marca_veh',\n",
    "            'sexo_pil', 'edad_pil', 'estado_pil', 'causa_acc'\n",
    "        ]\n",
    "    elif tipo_dataset == 'fallecidos_lesionados':\n",
    "        columnas_relevantes = [\n",
    "            'num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu',\n",
    "            'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu',\n",
    "            'tipo_veh', 'causa_acc', 'sexo_pil', 'edad_pil',\n",
    "            'fallecidos_lesionados', 'tipo_eve'\n",
    "        ]\n",
    "    else:\n",
    "        columnas_relevantes = []\n",
    "    \n",
    "    # Agregar columnas de metadatos\n",
    "    columnas_metadatos = ['archivo_origen', 'tipo_dataset']\n",
    "    \n",
    "    # Seleccionar solo columnas existentes\n",
    "    columnas_existentes = [col for col in df.columns if col in columnas_relevantes + columnas_metadatos]\n",
    "    \n",
    "    # Verificar que tenemos columnas minimas\n",
    "    columnas_minimas = ['depto_ocu']  # Ya no requerimos 'año' aquí porque lo extraeremos después\n",
    "    columnas_faltantes = [col for col in columnas_minimas if col not in columnas_existentes]\n",
    "    if columnas_faltantes:\n",
    "        print(f\"   Advertencia: Faltan columnas minimas {columnas_faltantes} en {tipo_dataset}\")\n",
    "    \n",
    "    # Si no hay columnas relevantes, usar todas las disponibles\n",
    "    if not columnas_existentes:\n",
    "        print(f\"   No se encontraron columnas relevantes, usando todas las columnas disponibles\")\n",
    "        columnas_existentes = list(dict.fromkeys(df.columns.tolist()))  # Eliminar duplicados\n",
    "    \n",
    "    # Crear nuevo DataFrame con columnas seleccionadas\n",
    "    df_limpio = df[columnas_existentes].copy()\n",
    "    \n",
    "    # Extraer el año desde 'archivo_origen' y crear la columna 'año'\n",
    "    if 'archivo_origen' in df_limpio.columns:\n",
    "        df_limpio['año'] = df_limpio['archivo_origen'].apply(extraer_ano_archivo)\n",
    "        print(f\"   - Columna 'año' extraída de 'archivo_origen'\")\n",
    "    \n",
    "    # Limpiar datos\n",
    "    registros_originales = len(df_limpio)\n",
    "    df_limpio = df_limpio.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   - Columnas originales: {len(df.columns)}\")\n",
    "    print(f\"   - Columnas finales: {len(df_limpio.columns)}\")\n",
    "    print(f\"   - Registros originales: {registros_originales}\")\n",
    "    print(f\"   - Registros despues de eliminar duplicados: {len(df_limpio)}\")\n",
    "    print(f\"   - Columnas seleccionadas: {list(df_limpio.columns)}\")\n",
    "    \n",
    "    return df_limpio\n",
    "\n",
    "def analizar_calidad_datos(df, nombre_dataset):\n",
    "    \"\"\"Analiza la calidad de los datos despues de la limpieza\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        print(f\"ERROR: {nombre_dataset}: Dataset vacio o no procesado\")\n",
    "        return\n",
    "    \n",
    "    print(f\"CALIDAD DE DATOS - {nombre_dataset}:\")\n",
    "    print(f\"   - Total registros: {len(df):,}\")\n",
    "    print(f\"   - Total columnas: {len(df.columns)}\")\n",
    "    \n",
    "    if 'año' in df.columns:\n",
    "        print(f\"   - Años disponibles: {sorted(df['año'].unique())}\")\n",
    "    \n",
    "    print(f\"   - Valores nulos por columna:\")\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Asegurarnos de que estamos trabajando con una sola columna\n",
    "            if col in df.columns:\n",
    "                nulos = df[col].isna().sum()\n",
    "                # Verificar si nulos es un escalar (numero) o una Serie\n",
    "                if hasattr(nulos, 'item'):  # Si es una Serie/array\n",
    "                    nulos = nulos.item()\n",
    "                \n",
    "                if pd.isna(nulos):\n",
    "                    nulos = 0\n",
    "                \n",
    "                if nulos > 0:\n",
    "                    porcentaje = (nulos / len(df)) * 100\n",
    "                    print(f\"     - {col}: {nulos:,} nulos ({porcentaje:.1f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"     - {col}: Error calculando nulos - {e}\")\n",
    "\n",
    "# Configuracion\n",
    "directorio_entrada = \"./datasets_unidos\"\n",
    "directorio_salida = \"./datasets_limpios\"\n",
    "\n",
    "Path(directorio_salida).mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Cargar datasets unidos\n",
    "    print(\"Cargando datasets unidos...\")\n",
    "    \n",
    "    hechos = pd.read_csv(f\"{directorio_entrada}/hechos_transito_completo.csv\")\n",
    "    vehiculos = pd.read_csv(f\"{directorio_entrada}/vehiculos_involucrados_completo.csv\")\n",
    "    fallecidos = pd.read_csv(f\"{directorio_entrada}/fallecidos_lesionados_completo.csv\")\n",
    "    \n",
    "    print(f\"Datasets cargados:\")\n",
    "    print(f\"   - Hechos: {len(hechos):,} registros, {len(hechos.columns)} columnas\")\n",
    "    print(f\"   - Vehiculos: {len(vehiculos):,} registros, {len(vehiculos.columns)} columnas\")\n",
    "    print(f\"   - Fallecidos: {len(fallecidos):,} registros, {len(fallecidos.columns)} columnas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cargando datasets: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Estandarizar nombres de columnas\n",
    "print(\"\\nEstandarizando nombres de columnas...\")\n",
    "hechos = estandarizar_nombres_columnas(hechos)\n",
    "vehiculos = estandarizar_nombres_columnas(vehiculos)\n",
    "fallecidos = estandarizar_nombres_columnas(fallecidos)\n",
    "\n",
    "print(\"   Nombres de columnas estandarizados\")\n",
    "\n",
    "# Eliminar columnas duplicadas\n",
    "print(\"\\nEliminando columnas duplicadas...\")\n",
    "hechos = eliminar_columnas_duplicadas(hechos)\n",
    "vehiculos = eliminar_columnas_duplicadas(vehiculos)\n",
    "fallecidos = eliminar_columnas_duplicadas(fallecidos)\n",
    "\n",
    "# Combinar columnas similares\n",
    "print(\"\\nCombinando columnas similares...\")\n",
    "hechos = combinar_columnas_similares(hechos)\n",
    "vehiculos = combinar_columnas_similares(vehiculos)\n",
    "fallecidos = combinar_columnas_similares(fallecidos)\n",
    "\n",
    "# Limpiar datasets\n",
    "print(\"\\nLimpiando y seleccionando columnas relevantes...\")\n",
    "hechos_limpio = limpiar_y_seleccionar_columnas(hechos, 'hechos_transito')\n",
    "vehiculos_limpio = limpiar_y_seleccionar_columnas(vehiculos, 'vehiculos_involucrados')\n",
    "fallecidos_limpio = limpiar_y_seleccionar_columnas(fallecidos, 'fallecidos_lesionados')\n",
    "\n",
    "# Analizar calidad\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALISIS DE CALIDAD POST-LIMPIEZA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analizar_calidad_datos(hechos_limpio, \"HECHOS_TRANSITO\")\n",
    "analizar_calidad_datos(vehiculos_limpio, \"VEHICULOS_INVOLUCRADOS\")\n",
    "analizar_calidad_datos(fallecidos_limpio, \"FALLECIDOS_LESIONADOS\")\n",
    "\n",
    "# Guardar datasets limpios\n",
    "print(f\"\\nGuardando datasets limpios...\")\n",
    "try:\n",
    "    hechos_limpio.to_csv(f\"{directorio_salida}/hechos_transito_limpio.csv\", index=False, encoding='utf-8-sig')\n",
    "    vehiculos_limpio.to_csv(f\"{directorio_salida}/vehiculos_involucrados_limpio.csv\", index=False, encoding='utf-8-sig')\n",
    "    fallecidos_limpio.to_csv(f\"{directorio_salida}/fallecidos_lesionados_limpio.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"Datasets limpios guardados en: {directorio_salida}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error guardando datasets: {e}\")\n",
    "\n",
    "# Mostrar estructura final\n",
    "print(f\"\\nESTRUCTURA FINAL:\")\n",
    "if hechos_limpio is not None:\n",
    "    print(f\"   Hechos transito: {list(hechos_limpio.columns)}\")\n",
    "if vehiculos_limpio is not None:\n",
    "    print(f\"   Vehiculos: {list(vehiculos_limpio.columns)}\")\n",
    "if fallecidos_limpio is not None:\n",
    "    print(f\"   Fallecidos/Lesionados: {list(fallecidos_limpio.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e3aea",
   "metadata": {},
   "source": [
    "Luego de hacer varias pruebas este fue el mejor resultado para el data set. Es importante destacar que no todas las columnas fueron procesadas exitosamene, se a reducido la dimensión de los sets de datos. Esto pasó principalmente porque entre los distintos años habían varias columnas que o bien tenían nombres similares o columnas que solo se encontraban en algunos años en específico. Realmente el set de datos no tiene un id que relacione a las tres aparte del numero de correlativo o numero de hecho. Algunas de las estrategias que hicimos fue juntar los datos de columnas duplicadas y también columnas que significaban prácticamente lo mismo. Aún después de esto hay algunas columnas con valores núlos, pero al menos la mayoría parece estar funcionando y debería de ser suficiente para poder analizar los 3 archivos por separados y en conjunto. Para hacer los joins se espera poder usar cosas como llaves compuetas con el año, fecha, dia,zona y demás como sugiere la guía. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff5d22",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab8_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
