{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbc598",
   "metadata": {},
   "source": [
    "# Análisis de accidentes con Spark\n",
    "\n",
    "En este laboratorio lo que se busca es poder encotrar información interesante y relevante con respecto a los accidentes que han ocurrido de los años de 2013 a 2023 en Guatemala. Principalmente tenemos 3 tipos de archivos: Fallecidos y Lesionados, Hechos de tránsito y Vehículos involucrados. Estos 3 archivos contienen información con respecto a accidentes automovílisticos. La idea es poder usar spark para poder analizar los eventos y las características relacionadas a este. Lo primero es juntar todos los archivos. Obtuvimos los datos de la página del Instituto Nacional de Estadística (INE): https://www.ine.gob.gt/bases-de-datos/accidentes-de-transito/. Descargando todos los archivos desde 2013 a 2023. El procesamiento se hizo en distintas etapas:\n",
    "\n",
    "1. Descargar todos los datos en un sav y si no esta disponible en ese formato, en excel.\n",
    "2. Juntar todos los dataframes en los 3 tipos de archivos descritos (fallecidos y lesionados, hechos de tránsito y vehículos involucrados)\n",
    "3. Analizar los datos y ver columnas con muchos valores vacíos, columnas repetidas entre otras situaciones que generen data innecesaria o redundante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdfe73",
   "metadata": {},
   "source": [
    "# Merge detodos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb63d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de unión y combinación...\n",
      "\n",
      "Procesando hechos_transito...\n",
      "Procesando: hechos_transito2013.sav\n",
      "Procesando: hechos_transito2014.sav\n",
      "Procesando: hechos_transito2015.xlsx\n",
      "Procesando: hechos_transito2016.sav\n",
      "Procesando: hechos_transito2017.sav\n",
      "Procesando: hechos_transito2018.sav\n",
      "Procesando: hechos_transito2019.sav\n",
      "Procesando: hechos_transito2020.sav\n",
      "Procesando: hechos_transito2021.sav\n",
      "Procesando: hechos_transito2022.sav\n",
      "Procesando: hechos_transito2023.sav\n",
      "hechos_transito: 76759 registros, 18 columnas\n",
      "\n",
      "Procesando vehiculos_involucrados...\n",
      "Procesando: vehiculos_involucrados2013.sav\n",
      "Procesando: vehiculos_involucrados2014.sav\n",
      "Procesando: vehiculos_involucrados2015.xlsx\n",
      "Procesando: vehiculos_involucrados2016.sav\n",
      "Procesando: vehiculos_involucrados2017.sav\n",
      "Procesando: vehiculos_involucrados2018.sav\n",
      "Procesando: vehiculos_involucrados2019.sav\n",
      "Procesando: vehiculos_involucrados2020.sav\n",
      "Procesando: vehiculos_involucrados2021.sav\n",
      "Procesando: vehiculos_involucrados2022.sav\n",
      "Procesando: vehiculos_involucrados2023.sav\n",
      "vehiculos_involucrados: 111988 registros, 19 columnas\n",
      "\n",
      "Procesando fallecidos_lesionados...\n",
      "Procesando: fallecidos_lesionados2013.sav\n",
      "Procesando: fallecidos_lesionados2014.sav\n",
      "Procesando: fallecidos_lesionados2015.xlsx\n",
      "Procesando: fallecidos_lesionados2016.sav\n",
      "Procesando: fallecidos_lesionados2017.sav\n",
      "Procesando: fallecidos_lesionados2018.sav\n",
      "Procesando: fallecidos_lesionados2019.sav\n",
      "Procesando: fallecidos_lesionados2020.sav\n",
      "Procesando: fallecidos_lesionados2021.sav\n",
      "Procesando: fallecidos_lesionados2022.sav\n",
      "Procesando: fallecidos_lesionados2023.sav\n",
      "fallecidos_lesionados: 109417 registros, 17 columnas\n",
      "Guardado: ./datasets_corregidos/hechos_transito_corregido.csv\n",
      "Guardado: ./datasets_corregidos/vehiculos_involucrados_corregido.csv\n",
      "Guardado: ./datasets_corregidos/fallecidos_lesionados_corregido.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def estandarizar_nombre_columna(col):\n",
    "    \"\"\"Estandariza un nombre de columna: minusculas, sin tildes, sin espacios\"\"\"\n",
    "    col = str(col).lower()\n",
    "    col = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    col = col.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
    "    return col\n",
    "\n",
    "def procesar_archivo_individual(archivo, tipo_dataset):\n",
    "    \"\"\"Procesa un archivo individual y estandariza sus columnas\"\"\"\n",
    "    print(f\"Procesando: {archivo.name}\")\n",
    "    \n",
    "    # Leer el archivo según su formato\n",
    "    if archivo.suffix == '.sav':\n",
    "        df, meta = pyreadstat.read_sav(archivo)\n",
    "    elif archivo.suffix == '.xlsx':\n",
    "        df = pd.read_excel(archivo)\n",
    "    else:\n",
    "        print(f\"Formato no soportado: {archivo.name}\")\n",
    "        return None\n",
    "    \n",
    "    # Estandarizar nombres de columnas\n",
    "    df.columns = [estandarizar_nombre_columna(col) for col in df.columns]\n",
    "    \n",
    "    # Extraer año del nombre del archivo\n",
    "    año = re.search(r'(\\d{4})', archivo.stem).group(1) if re.search(r'(\\d{4})', archivo.stem) else \"Desconocido\"\n",
    "    \n",
    "    # Agregar metadatos\n",
    "    df['archivo_origen'] = archivo.name\n",
    "    df['tipo_dataset'] = tipo_dataset\n",
    "    df['año'] = año\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combinar_columnas_en_dataframe(df):\n",
    "    \"\"\"Combina columnas similares dentro de un DataFrame\"\"\"\n",
    "    # Mapeo de columnas a consolidar\n",
    "    mapeo_combinacion = {\n",
    "        'num_hecho': ['num_hecho', 'num_correlativo', 'num_corre', 'núm_corre'],\n",
    "        'dia_ocu': ['dia_ocu', 'día_ocu'],\n",
    "        'mes_ocu': ['mes_ocu'],\n",
    "        'hora_ocu': ['hora_ocu'],\n",
    "        'dia_sem_ocu': ['dia_sem_ocu', 'día_sem_ocu'],\n",
    "        'depto_ocu': ['depto_ocu'],\n",
    "        'mupio_ocu': ['mupio_ocu'],\n",
    "        'areag_ocu': ['areag_ocu', 'area_geo_ocu', 'área_geo_ocu'],\n",
    "        'zona_ocu': ['zona_ocu', 'zona_ciudad'],\n",
    "        'sexo_pil': ['sexo_pil', 'sexo_con', 'sexo_per', 'sexo_víc'],\n",
    "        'edad_pil': ['edad_pil', 'edad_con', 'edad_per', 'edad_víc'],\n",
    "        'tipo_veh': ['tipo_veh'],\n",
    "        'color_veh': ['color_veh'],\n",
    "        'modelo_veh': ['modelo_veh'],\n",
    "        'causa_acc': ['causa_acc'],\n",
    "        'marca_veh': ['marca_veh'],\n",
    "        'estado_pil': ['estado_pil', 'estado_con'],\n",
    "        'fallecidos_lesionados': ['fallecidos_lesionados', 'fall_les'],\n",
    "        'tipo_eve': ['tipo_eve']\n",
    "    }\n",
    "    \n",
    "    for col_principal, columnas_similares in mapeo_combinacion.items():\n",
    "        # Buscar qué columnas similares existen en este DataFrame\n",
    "        columnas_existentes = [col for col in columnas_similares if col in df.columns]\n",
    "        \n",
    "        if len(columnas_existentes) > 0:\n",
    "            # Si la columna principal no existe, crear una nueva\n",
    "            if col_principal not in df.columns:\n",
    "                # Usar la primera columna existente como base\n",
    "                df[col_principal] = df[columnas_existentes[0]]\n",
    "                # Si hay más columnas, combinar\n",
    "                for col in columnas_existentes[1:]:\n",
    "                    mask = df[col_principal].isna() & df[col].notna()\n",
    "                    df.loc[mask, col_principal] = df.loc[mask, col]\n",
    "            else:\n",
    "                # Si la columna principal existe, combinar con las demás\n",
    "                for col in columnas_existentes:\n",
    "                    if col != col_principal:\n",
    "                        mask = df[col_principal].isna() & df[col].notna()\n",
    "                        df.loc[mask, col_principal] = df.loc[mask, col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def unir_y_combinar_datasets(directorio_data):\n",
    "    \"\"\"Une y combina todos los datasets\"\"\"\n",
    "    tipos_datasets = {\n",
    "        'hechos_transito': [],\n",
    "        'vehiculos_involucrados': [],\n",
    "        'fallecidos_lesionados': []\n",
    "    }\n",
    "    \n",
    "    # Identificar archivos por tipo\n",
    "    for archivo in Path(directorio_data).iterdir():\n",
    "        if archivo.suffix in ['.sav', '.xlsx']:\n",
    "            nombre = archivo.stem.lower()\n",
    "            if 'hechos_transito' in nombre:\n",
    "                tipos_datasets['hechos_transito'].append(archivo)\n",
    "            elif 'vehiculos_involucrados' in nombre:\n",
    "                tipos_datasets['vehiculos_involucrados'].append(archivo)\n",
    "            elif 'fallecidos_lesionados' in nombre:\n",
    "                tipos_datasets['fallecidos_lesionados'].append(archivo)\n",
    "    \n",
    "    datasets_finales = {}\n",
    "    \n",
    "    for tipo, archivos in tipos_datasets.items():\n",
    "        print(f\"\\nProcesando {tipo}...\")\n",
    "        dataframes = []\n",
    "        \n",
    "        for archivo in sorted(archivos):\n",
    "            df = procesar_archivo_individual(archivo, tipo)\n",
    "            if df is not None:\n",
    "                # Combinar columnas similares en este archivo individual\n",
    "                df = combinar_columnas_en_dataframe(df)\n",
    "                dataframes.append(df)\n",
    "        \n",
    "        if dataframes:\n",
    "            # Unir todos los dataframes del mismo tipo\n",
    "            df_final = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "            \n",
    "            # Seleccionar columnas relevantes según el tipo\n",
    "            if tipo == 'hechos_transito':\n",
    "                columnas_finales = [\n",
    "                    'num_hecho', 'dia_ocu', 'mes_ocu', 'dia_sem_ocu', 'hora_ocu',\n",
    "                    'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu', \n",
    "                    'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc',\n",
    "                    'sexo_pil', 'edad_pil', 'archivo_origen', 'tipo_dataset', 'año'\n",
    "                ]\n",
    "            elif tipo == 'vehiculos_involucrados':\n",
    "                columnas_finales = [\n",
    "                    'num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu',\n",
    "                    'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu',\n",
    "                    'tipo_veh', 'color_veh', 'modelo_veh', 'causa_acc',\n",
    "                    'marca_veh', 'estado_pil', 'sexo_pil', 'edad_pil',\n",
    "                    'archivo_origen', 'tipo_dataset', 'año'\n",
    "                ]\n",
    "            else:  # fallecidos_lesionados\n",
    "                columnas_finales = [\n",
    "                    'num_hecho', 'dia_ocu', 'mes_ocu', 'hora_ocu',\n",
    "                    'depto_ocu', 'mupio_ocu', 'areag_ocu', 'zona_ocu',\n",
    "                    'tipo_veh', 'causa_acc', 'sexo_pil', 'edad_pil',\n",
    "                    'fallecidos_lesionados', 'tipo_eve', 'archivo_origen', 'tipo_dataset', 'año'\n",
    "                ]\n",
    "            \n",
    "            # Seleccionar solo las columnas que existen\n",
    "            columnas_existentes = [col for col in columnas_finales if col in df_final.columns]\n",
    "            df_final = df_final[columnas_existentes]\n",
    "            \n",
    "            # Eliminar duplicados\n",
    "            df_final = df_final.drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "            datasets_finales[tipo] = df_final\n",
    "            print(f\"{tipo}: {len(df_final)} registros, {len(df_final.columns)} columnas\")\n",
    "    \n",
    "    return datasets_finales\n",
    "\n",
    "# Ejecutar la unión y combinación\n",
    "print(\"Iniciando proceso de unión y combinación...\")\n",
    "directorio_data = \"./data\"\n",
    "datasets_finales = unir_y_combinar_datasets(directorio_data)\n",
    "\n",
    "# Guardar los datasets resultantes\n",
    "directorio_salida = \"./datasets_corregidos\"\n",
    "Path(directorio_salida).mkdir(exist_ok=True)\n",
    "\n",
    "for tipo, df in datasets_finales.items():\n",
    "    archivo_salida = f\"{directorio_salida}/{tipo}_corregido.csv\"\n",
    "    df.to_csv(archivo_salida, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Guardado: {archivo_salida}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf3505",
   "metadata": {},
   "source": [
    "se puede ver que se han logrado juntar la información de todos los años, incluso teniendo archivos de distinto tipo. Ahora es encesario evaluar las columnas y ver cuales podrían no ser necesaríass o incluso problemáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa5ad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE CALIDAD COMPLETO - DATASETS CORREGIDOS\n",
      "================================================================================\n",
      "\n",
      "HECHOS_TRANSITO:\n",
      "   Total registros: 76,759\n",
      "   Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   Total columnas: 18\n",
      "\n",
      "   ANÁLISIS POR COLUMNA:\n",
      "     num_hecho:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 8218.0]\n",
      "      - Promedio: 3547.75\n",
      "     dia_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 31.0]\n",
      "      - Promedio: 15.62\n",
      "     mes_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 12.0]\n",
      "      - Promedio: 6.58\n",
      "     dia_sem_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 7.0]\n",
      "      - Promedio: 4.35\n",
      "     hora_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [0.0 - 99.0]\n",
      "      - Promedio: 13.72\n",
      "     depto_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 22.0]\n",
      "      - Promedio: 7.24\n",
      "     mupio_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [101.0 - 2217.0]\n",
      "      - Promedio: 729.95\n",
      "     areag_ocu:\n",
      "      - Nulos: 49,966 (65.1%)\n",
      "      - No nulos: 26,793\n",
      "      - Rango: [1.0 - 2.0]\n",
      "      - Promedio: 1.62\n",
      "     zona_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 72.52\n",
      "     tipo_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 11.29\n",
      "     color_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 32.69\n",
      "     modelo_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 76,759\n",
      "      - Rango: [1886.0 - 9999.0]\n",
      "      - Promedio: 7262.13\n",
      "     causa_acc:\n",
      "      - Nulos: 70,435 (91.8%)\n",
      "      - No nulos: 6,324\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 2.96\n",
      "     sexo_pil:\n",
      "      - Nulos: 57,930 (75.5%)\n",
      "      - No nulos: 18,829\n",
      "      - Rango: [1.0 - 9.0]\n",
      "      - Promedio: 2.61\n",
      "     edad_pil:\n",
      "      - Nulos: 57,930 (75.5%)\n",
      "      - No nulos: 18,829\n",
      "      - Rango: [11.0 - 999.0]\n",
      "      - Promedio: 328.91\n",
      "\n",
      "VEHICULOS_INVOLUCRADOS:\n",
      "   Total registros: 111,988\n",
      "   Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   Total columnas: 19\n",
      "\n",
      "   ANÁLISIS POR COLUMNA:\n",
      "     num_hecho:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 12796.0]\n",
      "      - Promedio: 5272.27\n",
      "     dia_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 31.0]\n",
      "      - Promedio: 15.65\n",
      "     mes_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 12.0]\n",
      "      - Promedio: 6.60\n",
      "     hora_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [0.0 - 99.0]\n",
      "      - Promedio: 13.70\n",
      "     depto_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 22.0]\n",
      "      - Promedio: 7.15\n",
      "     mupio_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [101.0 - 2217.0]\n",
      "      - Promedio: 720.55\n",
      "     areag_ocu:\n",
      "      - Nulos: 76,320 (68.2%)\n",
      "      - No nulos: 35,668\n",
      "      - Rango: [1.0 - 2.0]\n",
      "      - Promedio: 1.60\n",
      "     zona_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 71.85\n",
      "     tipo_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 8.95\n",
      "     color_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 29.36\n",
      "     modelo_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 9999.0]\n",
      "      - Promedio: 6275.58\n",
      "     causa_acc:\n",
      "      - Nulos: 105,665 (94.4%)\n",
      "      - No nulos: 6,323\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 1.44\n",
      "     marca_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 999.0]\n",
      "      - Promedio: 258.92\n",
      "     estado_pil:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 9.0]\n",
      "      - Promedio: 6.46\n",
      "     sexo_pil:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [1.0 - 9.0]\n",
      "      - Promedio: 2.01\n",
      "     edad_pil:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 111,988\n",
      "      - Rango: [3.0 - 999.0]\n",
      "      - Promedio: 256.20\n",
      "\n",
      "FALLECIDOS_LESIONADOS:\n",
      "   Total registros: 109,417\n",
      "   Años disponibles: ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
      "   Total columnas: 17\n",
      "\n",
      "   ANÁLISIS POR COLUMNA:\n",
      "     num_hecho:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 11668.0]\n",
      "      - Promedio: 5034.32\n",
      "     dia_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 31.0]\n",
      "      - Promedio: 15.66\n",
      "     mes_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 12.0]\n",
      "      - Promedio: 6.53\n",
      "     hora_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [0.0 - 99.0]\n",
      "      - Promedio: 13.58\n",
      "     depto_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 22.0]\n",
      "      - Promedio: 7.85\n",
      "     mupio_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [101.0 - 2217.0]\n",
      "      - Promedio: 790.87\n",
      "     areag_ocu:\n",
      "      - Nulos: 69,302 (63.3%)\n",
      "      - No nulos: 40,115\n",
      "      - Rango: [1.0 - 2.0]\n",
      "      - Promedio: 1.68\n",
      "     zona_ocu:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [0.0 - 99.0]\n",
      "      - Promedio: 75.16\n",
      "     tipo_veh:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 10.04\n",
      "     causa_acc:\n",
      "      - Nulos: 100,357 (91.7%)\n",
      "      - No nulos: 9,060\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 2.89\n",
      "     sexo_pil:\n",
      "      - Nulos: 8,990 (8.2%)\n",
      "      - No nulos: 100,427\n",
      "      - Rango: [1.0 - 9.0]\n",
      "      - Promedio: 1.30\n",
      "     edad_pil:\n",
      "      - Nulos: 8,990 (8.2%)\n",
      "      - No nulos: 100,427\n",
      "      - Rango: [0.0 - 999.0]\n",
      "      - Promedio: 103.70\n",
      "     fallecidos_lesionados:\n",
      "      - Nulos: 0 (0.0%)\n",
      "      - No nulos: 109,417\n",
      "      - Rango: [1.0 - 9.0]\n",
      "      - Promedio: 1.82\n",
      "     tipo_eve:\n",
      "      - Nulos: 9,060 (8.3%)\n",
      "      - No nulos: 100,357\n",
      "      - Rango: [1.0 - 99.0]\n",
      "      - Promedio: 2.65\n"
     ]
    }
   ],
   "source": [
    "# Análisis de calidad completo para todos los datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DE CALIDAD COMPLETO - DATASETS CORREGIDOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analizar_calidad_completo(df, nombre_dataset):\n",
    "    \"\"\"Analiza la calidad de todas las columnas importantes del dataset\"\"\"\n",
    "    print(f\"\\n{nombre_dataset.upper()}:\")\n",
    "    print(f\"   Total registros: {len(df):,}\")\n",
    "    print(f\"   Años disponibles: {sorted(df['año'].unique())}\")\n",
    "    print(f\"   Total columnas: {len(df.columns)}\")\n",
    "    \n",
    "    # Columnas a analizar (excluyendo metadatos)\n",
    "    columnas_analizar = [col for col in df.columns if col not in ['archivo_origen', 'tipo_dataset', 'año']]\n",
    "    \n",
    "    print(f\"\\n   ANÁLISIS POR COLUMNA:\")\n",
    "    for col in columnas_analizar:\n",
    "        if col in df.columns:\n",
    "            total_registros = len(df)\n",
    "            nulos = df[col].isna().sum()\n",
    "            porcentaje_nulos = (nulos / total_registros) * 100\n",
    "            \n",
    "            # Información adicional según el tipo de columna\n",
    "            if df[col].dtype in ['object', 'string']:\n",
    "                # Para columnas categóricas\n",
    "                valores_unicos = df[col].nunique()\n",
    "                valor_mas_comun = df[col].mode().iloc[0] if not df[col].mode().empty else \"N/A\"\n",
    "                print(f\"     {col}:\")\n",
    "                print(f\"      - Nulos: {nulos:,} ({porcentaje_nulos:.1f}%)\")\n",
    "                print(f\"      - Valores únicos: {valores_unicos}\")\n",
    "                print(f\"      - Valor más común: {valor_mas_comun}\")\n",
    "                \n",
    "            else:\n",
    "                # Para columnas numéricas\n",
    "                no_nulos = total_registros - nulos\n",
    "                if no_nulos > 0:\n",
    "                    stats = df[col].describe()\n",
    "                    print(f\"     {col}:\")\n",
    "                    print(f\"      - Nulos: {nulos:,} ({porcentaje_nulos:.1f}%)\")\n",
    "                    print(f\"      - No nulos: {no_nulos:,}\")\n",
    "                    print(f\"      - Rango: [{stats['min']:.1f} - {stats['max']:.1f}]\")\n",
    "                    print(f\"      - Promedio: {stats['mean']:.2f}\")\n",
    "                else:\n",
    "                    print(f\"     {col}: TODOS LOS VALORES SON NULOS\")\n",
    "\n",
    "# Aplicar análisis a todos los datasets\n",
    "for tipo, df in datasets_finales.items():\n",
    "    analizar_calidad_completo(df, tipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c808e0",
   "metadata": {},
   "source": [
    "Viendo estos resultados es claro que huberion dificultades para juntar todos los datos al principio. Había más de 40 columnnas en los 3 archivos y varias de esas eran muy similares. Es más hay algunas columnas que son literalmente lo mismo, pero debido a ortografía (espacios, tildes, diferencias en el nombre) hay columnas que se marcan como diferente. Sabiendo esto es necesario limitar la selección de datos y jutnar la información redundante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e3aea",
   "metadata": {},
   "source": [
    "Luego de hacer varias pruebas este fue el mejor resultado para el data set. Es importante destacar que no todas las columnas fueron procesadas exitosamene, se a reducido la dimensión de los sets de datos. Esto pasó principalmente porque entre los distintos años habían varias columnas que o bien tenían nombres similares o columnas que solo se encontraban en algunos años en específico. Realmente el set de datos no tiene un id que relacione a las tres aparte del numero de correlativo o numero de hecho. Algunas de las estrategias que hicimos fue juntar los datos de columnas duplicadas y también columnas que significaban prácticamente lo mismo. Aún después de esto hay algunas columnas con valores núlos, pero al menos la mayoría parece estar funcionando y debería de ser suficiente para poder analizar los 3 archivos por separados y en conjunto. Para hacer los joins se espera poder usar cosas como llaves compuetas con el año, fecha, dia,zona y demás como sugiere la guía. la columna que tiene mayor cantidad de vlaores vacíos es la de causa_acc pero esperemos pueda complementarse con tipo_eve. aunque no todas las tablas tienen ambos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab8_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
